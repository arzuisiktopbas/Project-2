{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/freddieeisa/anaconda3/envs/pyvizenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "from newsapi import NewsApiClient\n",
    "from finvizfinance.screener.overview import Overview\n",
    "import alpaca_trade_api as tradeapi\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f147b186a1504298a5fca6370fdcfe78\n"
     ]
    }
   ],
   "source": [
    "# Read your api key environment variable\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"news_api\")\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a newsapi client\n",
    "newsapi = NewsApiClient(api_key=os.environ[\"news_api\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] loading page [##############################] 1/1 \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Country</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>P/E</th>\n",
       "      <th>Price</th>\n",
       "      <th>Change</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NEPT</th>\n",
       "      <td>Neptune Wellness Solutions Inc.</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Drug Manufacturers - Specialty &amp; Generic</td>\n",
       "      <td>Canada</td>\n",
       "      <td>253810000.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>44087769.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRN</th>\n",
       "      <td>Western Copper and Gold Corporation</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Other Industrial Metals &amp; Mining</td>\n",
       "      <td>Canada</td>\n",
       "      <td>229160000.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>3358816.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Company           Sector  \\\n",
       "Ticker                                                         \n",
       "NEPT        Neptune Wellness Solutions Inc.       Healthcare   \n",
       "WRN     Western Copper and Gold Corporation  Basic Materials   \n",
       "\n",
       "                                        Industry Country   Market Cap   P/E  \\\n",
       "Ticker                                                                        \n",
       "NEPT    Drug Manufacturers - Specialty & Generic  Canada  253810000.0  None   \n",
       "WRN             Other Industrial Metals & Mining  Canada  229160000.0  None   \n",
       "\n",
       "        Price  Change      Volume  \n",
       "Ticker                             \n",
       "NEPT     1.96  0.1667  44087769.0  \n",
       "WRN      1.69  0.1818   3358816.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foverview = Overview()\n",
    "\n",
    "filters_dict = {\n",
    "    'Performance':'Today Up',\n",
    "    'Price':\"Under $2\", \n",
    "    'Relative Volume': \"Over 5\"\n",
    "}\n",
    "foverview.set_filter(filters_dict=filters_dict)\n",
    "\n",
    "tickers = foverview.ScreenerView()\n",
    "#tickers.sort_values(by='Volume', inplace=True, ascending=False)\n",
    "tickers.set_index('Ticker', inplace=True )\n",
    "tickers.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(company):\n",
    "    stock_headlines = ( newsapi.get_everything(\n",
    "    q=company,\n",
    "    language='en',\n",
    "    sort_by='relevancy'))\n",
    "\n",
    "    sentiments = []\n",
    "\n",
    "    for articles in stock_headlines['articles']:\n",
    "        try:\n",
    "            text = articles['content']\n",
    "            results = analyzer.polarity_scores(text)\n",
    "            compound = results['compound']\n",
    "            pos = results['pos']\n",
    "            neu = results['neu']\n",
    "            neg = results['neg'] \n",
    "\n",
    "            #put data into sentiments\n",
    "            sentiments.append({\n",
    "                'text':text,\n",
    "                'Compound':compound,\n",
    "                'Positive':pos,\n",
    "                'Negative':neg,\n",
    "                'Neutral':neu})\n",
    "\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    fininal_sentiments = pd.DataFrame(sentiments)\n",
    "    if not fininal_sentiments.empty:\n",
    "        return(fininal_sentiments[\"Positive\"].mean())\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1689285714285714\n",
      "NEPT does meets our sentiment requirement\n",
      "0.0511\n",
      "WRN does meets our sentiment requirement\n"
     ]
    }
   ],
   "source": [
    "ticker_sentiment = [] \n",
    "for ticker in tickers.index: \n",
    "    sentiment = get_sentiment(tickers['Company'][ticker])\n",
    "    print(sentiment)\n",
    "    if float(sentiment) >= 0.05:\n",
    "        ticker_sentiment.append(ticker)\n",
    "        print(f\"{ticker} does meets our sentiment requirement\")\n",
    "    else:\n",
    "        print(f\"{ticker} does not meet our sentiment requirement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "api = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version = \"v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set time frame for reporting stocks\n",
    "today = datetime.date.today()\n",
    "tomorrow = today + datetime.timedelta(days = 2) \n",
    "start_time = tomorrow.strftime(\"%b %d %Y\") + \" 10:00AM\"\n",
    "next_day = today + datetime.timedelta(days = 1) \n",
    "end_time = next_day.strftime(\"%b %d %Y\") + \" 1:00PM\"\n",
    "\n",
    "date_from = datetime.datetime.strptime(\n",
    "    start_time, '%b %d %Y %I:%M%p')\n",
    "date_to = datetime.datetime.strptime(\n",
    "    end_time, '%b %d %Y %I:%M%p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "internal server error occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pyvizenv/lib/python3.7/site-packages/alpaca_trade_api/rest.py\u001b[0m in \u001b[0;36m_one_request\u001b[0;34m(self, method, url, opts, retry)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://data.alpaca.markets/v1/bars/1D?symbols=NEPT%2CWRN&start=2018-05-01T00%3A00%3A00-04%3A00&end=2021-02-21T00%3A00%3A00-05%3A00",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b70ac9fc7dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtimeframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m ).df\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyvizenv/lib/python3.7/site-packages/alpaca_trade_api/rest.py\u001b[0m in \u001b[0;36mget_barset\u001b[0;34m(self, symbols, timeframe, limit, start, end, after, until)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muntil\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'until'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/bars/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBarSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyvizenv/lib/python3.7/site-packages/alpaca_trade_api/rest.py\u001b[0m in \u001b[0;36mdata_get\u001b[0;34m(self, path, data)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mbase_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         return self._request(\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyvizenv/lib/python3.7/site-packages/alpaca_trade_api/rest.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, path, data, base_url, api_version)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_one_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRetryException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mretry_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry_wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyvizenv/lib/python3.7/site-packages/alpaca_trade_api/rest.py\u001b[0m in \u001b[0;36m_one_request\u001b[0;34m(self, method, url, opts, retry)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'code'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: internal server error occurred"
     ]
    }
   ],
   "source": [
    "# Set timeframe to '1D'\n",
    "timeframe = \"1D\"\n",
    "\n",
    "#Set timeframe for last 2 years\n",
    "start_date = pd.Timestamp(\"2018-05-01\", tz=\"America/New_York\").isoformat()\n",
    "end_date = pd.Timestamp(next_day, tz=\"America/New_York\").isoformat()\n",
    "\n",
    "#Get ticker data\n",
    "df_ticker = api.get_barset(\n",
    "    ticker_sentiment,\n",
    "    timeframe,\n",
    "    start=start_date,\n",
    "    end=end_date\n",
    ").df\n",
    "\n",
    "df_ticker.dropna(inplace=True)\n",
    "df_ticker.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ticker=df_ticker.loc[:, (slice(None), ['volume','close'])]\n",
    "#df_ticker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticker.columns = ['_'.join(col) for col in df_ticker.columns]\n",
    "df_ticker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ticker.iloc[:, [0,2]].plot()\n",
    "df_ticker.iloc[:, [3,8]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticker_1=df_ticker.iloc[:, [0,1,2,3,4]]\n",
    "df_ticker_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "delimiters = '_'\n",
    "matchPattern = ''.join(map(re.escape, delimiters))\n",
    "df_ticker_1.columns = [re.split(matchPattern, i)[1] for i in df_ticker_1.columns ]\n",
    "df_ticker_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_1 = df_ticker_1.iloc[:, [3]].pct_change() * 100\n",
    "returns_1.plot(title=\"Returns\",legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticker_1['returns'] = returns_1.copy()\n",
    "df_ticker_1 = df_ticker_1.dropna()\n",
    "df_ticker_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forming classes for classification: -1 for losing day. +1 for winning day\n",
    "df_ticker_1['target'] = np.where(df_ticker_1.returns.shift(-1) > 0, 1, -1)\n",
    "df_ticker_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count target values\n",
    "df_ticker_1['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['volume','returns']\n",
    "labels = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for volume and returns\n",
    "plt.figure(figsize=(8,3))\n",
    "sns.scatterplot(x=df_ticker_1['returns'],y=df_ticker_1['volume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "features_list = []\n",
    "\n",
    "# Standard deviation calculation\n",
    "for i in range(5, 20, 5):\n",
    "    col_name = 'std_' + str(i)\n",
    "    df_ticker_1[col_name] = df_ticker_1['close'].rolling(window=i).std()\n",
    "    features_list.append(col_name)\n",
    "    \n",
    "# Mean calculation\n",
    "for i in range(10, 30, 5):\n",
    "    col_name = 'ma_' + str(i)\n",
    "    df_ticker_1[col_name] = df_ticker_1['close'].rolling(window=i).mean()\n",
    "    features_list.append(col_name)\n",
    "    \n",
    "# % change calculation\n",
    "for i in range(3, 12, 3):\n",
    "    col_name = 'pct_' + str(i)\n",
    "    df_ticker_1[col_name] = df_ticker_1['close'].pct_change().rolling(window=i).sum()\n",
    "    features_list.append(col_name)\n",
    "    \n",
    "# volume mean calculation\n",
    "col_name = 'vma_4'\n",
    "df_ticker_1[col_name] = df_ticker_1['volume'].rolling(4).mean()\n",
    "features_list.append(col_name)\n",
    "\n",
    "# intraday calculation\n",
    "col_name = 'co'\n",
    "df_ticker_1[col_name] = df_ticker_1['close'] - df_ticker_1['open']\n",
    "features_list.append(col_name)\n",
    "\n",
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Nans\n",
    "df_ticker_1.dropna(inplace=True)\n",
    "\n",
    "#taking a peek at features\n",
    "df_ticker_1[features_list+['target']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train_test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df_ticker_1[features_list].iloc[:-1],\n",
    "                                                   df_ticker_1.iloc[:-1]['target'],\n",
    "                                                   test_size = 0.2)\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X_train[['std_5','ma_10','vma_4']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "sns.pairplot(X_train_scaled_df[['std_5','ma_10','vma_4']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_1 = LogisticRegression()\n",
    "\n",
    "# train model\n",
    "model_1.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some predictions\n",
    "Y_pred_train_1 = model_1.predict(X_train_scaled)\n",
    "\n",
    "Y_pred_1 = model_1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "print(\"Logistic Regression Model train accuracy:\", accuracy_score(Y_train, Y_pred_train_1))\n",
    "print(\"Logistic Regression Model test accuracy:\", accuracy_score(Y_test, Y_pred_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred_1)\n",
    "df = pd.DataFrame(cm, index=['Short', 'Long'], columns=['Short', 'Long'])\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(df,annot=True, fmt='g')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title( \"Logistic Regression Confusion Matrix\", fontsize=15, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(Y_test, Y_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_2 = GradientBoostingClassifier(n_estimators=10, max_features='auto')\n",
    "\n",
    "# train model\n",
    "model_2.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# make some predictions\n",
    "Y_pred_train_2 = model_2.predict(X_train_scaled)\n",
    "Y_pred_2 = model_2.predict(X_test_scaled)\n",
    "\n",
    "# accuracy\n",
    "print(\"Gradient Boosting Classifier Model train accuracy:\", accuracy_score(Y_train, Y_pred_train_2))\n",
    "print(\"Gradient Boosting Classifier Model test accuracy:\", accuracy_score(Y_test, Y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred_2)\n",
    "df = pd.DataFrame(cm, index=['Short', 'Long'], columns=['Short', 'Long'])\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(df,annot=True, fmt='g')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title( \"Gradient Boosting Classifier Confusion Matrix\", fontsize=15, fontweight=\"bold\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(Y_test, Y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-pyvizenv] *",
   "language": "python",
   "name": "conda-env-anaconda3-pyvizenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
